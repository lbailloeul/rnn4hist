#!/usr/bin/env python3
"""
Large-scale SPS Histogram Dataset Generator with SVM Labeling
============================================================
Generates 10k histograms with varied parameters and uses pre-trained SVM for automatic labeling.

Usage:
    python data_generator.py --num_samples 10000 --output_dir dataset

Features:
    - Parameter sampling from learned distributions
    - ROOT histogram generation via Python
    - SVM automatic labeling (92.4% accuracy)
    - PNG image export for CNN training
    - Parameter variance based on real data analysis
"""

import numpy as np
import pandas as pd
import pickle
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from sklearn.preprocessing import StandardScaler
import sys
import os
import math
from datetime import datetime

# Try to import ROOT, fallback if not available
try:
    import ROOT
    ROOT.gROOT.SetBatch(True)  # Run in batch mode
    has_root = True
except ImportError:
    print("âš ï¸  ROOT not available, using matplotlib simulation")
    has_root = False

class SPSDataGenerator:
    def __init__(self, svm_model_path=None, output_dir='dataset', num_samples=10000):
        """Initialize the SPS data generator"""
        self.output_dir = Path(output_dir)
        self.num_samples = num_samples
        self.svm_model = None
        self.scaler = None
        
        # Create output directories
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / 'good').mkdir(exist_ok=True)
        (self.output_dir / 'bad').mkdir(exist_ok=True)
        
        # Parameter ranges from analysis (Sigma_Gain, Sigma_0, Mu_p)
        self.param_ranges = {
            'Sigma_Gain': {'min': 0.5, 'max': 6.0, 'mean': 3.3, 'std': 1.5},
            'Sigma_0': {'min': 0.3, 'max': 3.0, 'mean': 1.7, 'std': 0.8},
            'Mu_p': {'min': 0.1, 'max': 3.5, 'mean': 1.9, 'std': 1.0}
        }
        
        # Fixed GP parameters (from analysis)
        self.fixed_params = {
            'lambda': 0.1,      # Crosstalk
            'gain': 12.2,       # Truth gain from analysis
            'mu_0': 90.0,       # Position of 0 peak
            'num_events': 3000, # Events per histogram
            'npeaks': 7,        # Number of photoelectron peaks
            'hist_bins': 300,   # Histogram bins
            'hist_range': (0, 300)  # ADU range
        }
        
        # Load SVM model if provided, or try to load from data-analysis
        if svm_model_path:
            self.load_svm_model(svm_model_path)
        else:
            print("ğŸ” No SVM model path provided, attempting to load from data-analysis...")
            self.load_svm_model(None)
            
    def load_svm_model(self, model_path):
        """Load pre-trained SVM model and scaler"""
        try:
            if model_path:
                # Load from specific file (not implemented yet)
                print(f"ğŸ”„ Loading SVM model from {model_path}...")
                # TODO: Implement loading from pickle file
                print("âŒ Loading from file not implemented yet")
                return
            
            # Load from data-analysis results
            sys.path.append('../data-analysis')
            from analyze import SPSAnalyzer
            
            print("ğŸ”„ Loading SVM model from analysis results...")
            analyzer = SPSAnalyzer()
            analyzer.load_data()
            analyzer.perform_clustering_analysis()
            classifier_results = analyzer.evaluate_cluster_purity_and_classifiers()
            
            if classifier_results:
                self.svm_model = classifier_results['classifiers']['svm']['model']
                self.scaler = classifier_results['scaler']
                print(f"âœ… SVM model loaded (accuracy: {classifier_results['classifiers']['svm']['metrics']['accuracy']:.3f})")
            else:
                print("âŒ Failed to load SVM model")
                
        except Exception as e:
            print(f"âŒ Error loading SVM model: {e}")
            self.svm_model = None
            
    def sample_parameters(self, n_samples):
        """Sample parameters from learned distributions"""
        params = {}
        
        for param_name, param_info in self.param_ranges.items():
            # Use truncated normal distribution
            samples = np.random.normal(param_info['mean'], param_info['std'], n_samples)
            # Clip to realistic bounds
            samples = np.clip(samples, param_info['min'], param_info['max'])
            params[param_name] = samples
            
        return pd.DataFrame(params)
        
    def generalized_poisson(self, x, lambda_ct, gain, sigma_gain, mu_avg, mu_0, sigma_0, N, npeaks):
        """Generalized Poisson distribution for SPS modeling"""
        fitval = np.zeros_like(x)
        
        for k in range(int(npeaks) + 1):
            uk = mu_avg + k * lambda_ct
            G = mu_avg * np.power(uk, k - 1) * np.exp(-uk) / math.factorial(k) if k > 0 else np.exp(-mu_avg)
            sk2 = sigma_0**2 + k * sigma_gain**2
            dx = x - mu_0 - k * gain
            fitval += N * G * np.exp(-0.5 * dx**2 / sk2) / np.sqrt(2 * np.pi) / np.sqrt(sk2)
            
        return fitval
        
    def generate_histogram_matplotlib(self, params_row):
        """Generate histogram using matplotlib (fallback when ROOT not available)"""
        sigma_gain, sigma_0, mu_p = params_row['Sigma_Gain'], params_row['Sigma_0'], params_row['Mu_p']
        
        # Generate random samples from GP distribution
        x_range = np.linspace(*self.fixed_params['hist_range'], self.fixed_params['hist_bins'])
        pdf = self.generalized_poisson(
            x_range,
            self.fixed_params['lambda'],
            self.fixed_params['gain'],
            sigma_gain,
            mu_p,
            self.fixed_params['mu_0'],
            sigma_0,
            1.0,  # Normalized
            self.fixed_params['npeaks']
        )
        
        # Normalize PDF
        pdf = pdf / np.sum(pdf)
        
        # Sample from distribution
        samples = np.random.choice(x_range, size=self.fixed_params['num_events'], p=pdf)
        
        # Create histogram
        hist_counts, bin_edges = np.histogram(samples, bins=self.fixed_params['hist_bins'], 
                                            range=self.fixed_params['hist_range'])
        
        return hist_counts, bin_edges
        
    def generate_histogram_root(self, params_row):
        """Generate histogram using ROOT (preferred method)"""
        sigma_gain, sigma_0, mu_p = params_row['Sigma_Gain'], params_row['Sigma_0'], params_row['Mu_p']
        
        # Create GP function
        func_gp = ROOT.TF1("func_gp", self.root_generpoiss, *self.fixed_params['hist_range'], 8)
        func_gp.SetParameters(
            self.fixed_params['lambda'],     # lambda
            self.fixed_params['gain'],       # gain
            sigma_gain,                      # sigma_gain
            mu_p,                           # mu_p
            self.fixed_params['mu_0'],       # mu_0
            sigma_0,                        # sigma_0
            self.fixed_params['num_events'], # N
            self.fixed_params['npeaks']      # npeaks
        )
        
        # Generate histogram
        hist = ROOT.TH1F(f"hist_{np.random.randint(0, 1000000)}", "SPS Histogram", 
                        self.fixed_params['hist_bins'], *self.fixed_params['hist_range'])
        hist.FillRandom("func_gp", self.fixed_params['num_events'])
        
        # Convert to numpy
        hist_counts = np.array([hist.GetBinContent(i) for i in range(1, hist.GetNbinsX() + 1)])
        bin_edges = np.array([hist.GetBinLowEdge(i) for i in range(1, hist.GetNbinsX() + 2)])
        
        # Clean up
        del hist
        del func_gp
        
        return hist_counts, bin_edges
        
    @staticmethod
    def root_generpoiss(x, p):
        """ROOT-compatible generalized Poisson function"""
        xx = x[0]
        lambda_ct = p[0]
        gain = p[1]
        sigma_gain = p[2] 
        mu_avg = p[3]
        mu_0 = p[4]
        sigma_0 = p[5]
        N = p[6]
        npeaks = p[7]
        
        fitval = 0.0
        for k in range(int(npeaks) + 1):
            uk = mu_avg + k * lambda_ct
            if k == 0:
                G = ROOT.TMath.Exp(-mu_avg)
            else:
                G = mu_avg * ROOT.TMath.Power(uk, k - 1) * ROOT.TMath.Exp(-uk) / ROOT.TMath.Factorial(k)
            
            sk2 = sigma_0*sigma_0 + k * sigma_gain*sigma_gain
            dx = xx - mu_0 - k * gain
            fitval += N * G * ROOT.TMath.Exp(-0.5 * dx*dx / sk2) / ROOT.TMath.Sqrt(2 * ROOT.TMath.Pi()) / ROOT.TMath.Sqrt(sk2)
            
        return fitval
        
    def save_histogram_root(self, hist_counts, bin_edges, filepath, params_row, label=None):
        """Save histogram as ROOT file for CNN training"""
        try:
            if not hasattr(self, 'ROOT_available') or not self.ROOT_available:
                # Fallback to numpy arrays saved as .npz files
                self.save_histogram_numpy(hist_counts, bin_edges, filepath, params_row, label)
                return
                
            import ROOT
            
            # Create ROOT file
            root_file = ROOT.TFile(str(filepath), "RECREATE")
            
            # Create histogram
            hist_name = f"hist_{filepath.stem}"
            hist = ROOT.TH1D(hist_name, f"SPS Histogram", len(bin_edges)-1, bin_edges[0], bin_edges[-1])
            
            # Fill histogram with bin contents
            for i, count in enumerate(hist_counts):
                hist.SetBinContent(i+1, count)  # ROOT bins start at 1
            
            # Set histogram metadata
            hist.SetTitle(f"Ïƒ_G={params_row['Sigma_Gain']:.2f}, Ïƒ_0={params_row['Sigma_0']:.2f}, Î¼_p={params_row['Mu_p']:.2f}")
            hist.GetXaxis().SetTitle("ADU")
            hist.GetYaxis().SetTitle("Counts")
            
            # Add label information as user info
            if label is not None:
                hist.GetListOfFunctions().Add(ROOT.TNamed("label", str(label)))
                hist.GetListOfFunctions().Add(ROOT.TNamed("label_text", "GOOD" if label == 1 else "BAD"))
            
            # Write and close
            hist.Write()
            root_file.Close()
            
        except Exception as e:
            print(f"âš ï¸ ROOT save failed, using numpy fallback: {e}")
            self.save_histogram_numpy(hist_counts, bin_edges, filepath, params_row, label)
    
    def save_histogram_numpy(self, hist_counts, bin_edges, filepath, params_row, label=None):
        """Fallback: Save histogram as numpy arrays"""
        # Change extension to .npz
        npz_filepath = filepath.with_suffix('.npz')
        
        # Save histogram data and metadata
        save_data = {
            'bin_counts': hist_counts,
            'bin_edges': bin_edges,
            'sigma_gain': params_row['Sigma_Gain'],
            'sigma_0': params_row['Sigma_0'],
            'mu_p': params_row['Mu_p']
        }
        
        if label is not None:
            save_data['label'] = label
            
        np.savez_compressed(npz_filepath, **save_data)
        
    def generate_dataset(self):
        """Generate complete dataset with SVM labeling"""
        print(f"ğŸš€ Generating {self.num_samples} histograms...")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        
        # Sample parameters
        print("ğŸ² Sampling parameters...")
        params_df = self.sample_parameters(self.num_samples)
        
        # Predict labels if SVM is available
        labels = None
        if self.svm_model and self.scaler:
            print("ğŸ” Predicting labels with SVM...")
            X_scaled = self.scaler.transform(params_df[['Sigma_Gain', 'Sigma_0', 'Mu_p']].values)
            labels = self.svm_model.predict(X_scaled)
            probabilities = self.svm_model.predict_proba(X_scaled)[:, 1]
            
            good_count = np.sum(labels == 1)
            bad_count = np.sum(labels == 0)
            print(f"   Predicted: {good_count} GOOD ({good_count/self.num_samples*100:.1f}%), {bad_count} BAD ({bad_count/self.num_samples*100:.1f}%)")
            
            # Add to dataframe
            params_df['Label'] = labels
            params_df['Probability'] = probabilities
        
        # Generate histograms
        print("ğŸ“Š Generating histograms...")
        metadata = []
        
        for i in range(self.num_samples):
            if (i + 1) % 1000 == 0:
                print(f"   Progress: {i + 1}/{self.num_samples} ({(i + 1)/self.num_samples*100:.1f}%)")
                
            params_row = params_df.iloc[i]
            
            # Generate histogram
            if has_root:
                hist_counts, bin_edges = self.generate_histogram_root(params_row)
            else:
                hist_counts, bin_edges = self.generate_histogram_matplotlib(params_row)
            
            # Determine output directory and filename
            if labels is not None:
                label = int(labels[i])
                subdir = 'good' if label == 1 else 'bad'
                # Use .root extension if ROOT available, otherwise .npz
                file_ext = '.root' if has_root else '.npz'
                filepath = self.output_dir / subdir / f'hist_{i:06d}{file_ext}'
            else:
                file_ext = '.root' if has_root else '.npz'
                filepath = self.output_dir / f'hist_{i:06d}{file_ext}'
                label = None
            
            # Save histogram as ROOT file or numpy array
            self.save_histogram_root(hist_counts, bin_edges, filepath, params_row, label)
            
            # Save metadata
            metadata_row = {
                'filename': filepath.name,
                'Sigma_Gain': params_row['Sigma_Gain'],
                'Sigma_0': params_row['Sigma_0'],
                'Mu_p': params_row['Mu_p'],
                'label': label,
                'probability': params_row.get('Probability', None),
                'subdir': subdir if labels is not None else 'unlabeled'
            }
            metadata.append(metadata_row)
        
        # Save metadata
        metadata_df = pd.DataFrame(metadata)
        metadata_path = self.output_dir / 'metadata.csv'
        metadata_df.to_csv(metadata_path, index=False)
        
        print(f"\nâœ… Dataset generation complete!")
        print(f"ğŸ“Š Generated {self.num_samples} histograms")
        if labels is not None:
            print(f"ğŸ“ Saved to: {self.output_dir}/good/ and {self.output_dir}/bad/")
            print(f"ğŸ“‹ Class distribution: {good_count} GOOD, {bad_count} BAD")
        else:
            print(f"ğŸ“ Saved to: {self.output_dir}/")
        print(f"ğŸ“„ Metadata saved to: {metadata_path}")
        
        return metadata_df

def main():
    """Main data generation function"""
    parser = argparse.ArgumentParser(description='SPS Dataset Generator with SVM Labeling')
    parser.add_argument('--num_samples', type=int, default=10000, help='Number of histograms to generate')
    parser.add_argument('--output_dir', default='dataset', help='Output directory')
    parser.add_argument('--svm_model', help='Path to SVM model (optional, will load from analysis)')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    args = parser.parse_args()
    
    # Set random seed
    np.random.seed(args.seed)
    
    print("ğŸ§ª SPS Large-scale Dataset Generator")
    print("=" * 50)
    print(f"ğŸ“Š Samples: {args.num_samples}")
    print(f"ğŸ“ Output: {args.output_dir}")
    print(f"ğŸ¯ SVM Model: {'From analysis' if not args.svm_model else args.svm_model}")
    
    # Initialize generator
    generator = SPSDataGenerator(
        svm_model_path=args.svm_model,
        output_dir=args.output_dir,
        num_samples=args.num_samples
    )
    
    # Generate dataset
    metadata = generator.generate_dataset()
    
    # Print summary statistics
    if 'label' in metadata and metadata['label'].notna().any():
        print(f"\nğŸ“ˆ Final Statistics:")
        print(f"   Total samples: {len(metadata)}")
        print(f"   GOOD samples: {np.sum(metadata['label'] == 1)}")
        print(f"   BAD samples: {np.sum(metadata['label'] == 0)}")
        if 'probability' in metadata:
            print(f"   Avg SVM confidence: {metadata['probability'].mean():.3f}")
            print(f"   High confidence (>0.9): {np.sum((metadata['probability'] > 0.9) | (metadata['probability'] < 0.1))}")

if __name__ == "__main__":
    main()